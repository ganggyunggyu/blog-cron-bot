import { crawlWithRetry, randomDelay } from '../../crawler';
import { extractPopularItems } from '../../parser';
import { matchBlogs } from '../../matcher';
import { DetailedLogBuilder } from '../../logs/detailed-log';
import { progressLogger } from '../../logs/progress-logger';
import { CRAWL_CONFIG } from '../../constants';
import { logger } from '../logger';
import { getAllowAnyBlog } from './allow-any-blog';
import { KeywordDoc, KeywordType, CrawlCaches, UpdateFunction } from './types';
import { extractRestaurantName } from './keyword-classifier';
import { crawlMultiPagesPlaywright } from '../playwright-crawler';

interface CrawlResult {
  items: any[];
  isPopular: boolean;
  uniqueGroupsSize: number;
  topicNamesArray: string[];
}

export const getCrawlResult = async (
  searchQuery: string,
  keywordDoc: KeywordDoc,
  query: string,
  globalIndex: number,
  totalKeywords: number,
  keywordStartTime: number,
  keywordType: KeywordType,
  caches: CrawlCaches,
  logBuilder: DetailedLogBuilder,
  updateFunction: UpdateFunction,
  maxPages: number = 1
): Promise<CrawlResult | null> => {
  const { crawlCache, itemsCache, matchQueueMap, htmlStructureCache } = caches;

  let items: any[];
  let isPopular: boolean;
  let uniqueGroupsSize: number;
  let topicNamesArray: string[] = [];

  if (!crawlCache.has(searchQuery)) {
    try {
      let html: string;

      if (maxPages > 1) {
        const htmls = await crawlMultiPagesPlaywright(searchQuery, maxPages);
        html = htmls[0];

        const allItems: any[] = [];
        const seenLinks = new Set<string>();

        htmls.forEach((pageHtml, pageIndex) => {
          const pageNumber = pageIndex + 1;
          const pageItems = extractPopularItems(pageHtml);
          for (const item of pageItems) {
            if (!seenLinks.has(item.link)) {
              seenLinks.add(item.link);
              allItems.push({ ...item, page: pageNumber });
            }
          }
        });

        items = allItems;
        logger.info(`üìÑ ${maxPages}ÌéòÏù¥ÏßÄ ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å: ${items.length}Í∞ú ÏïÑÏù¥ÌÖú`);
      } else {
        html = await crawlWithRetry(searchQuery, CRAWL_CONFIG.maxRetries);
        items = extractPopularItems(html);
      }

      const allowAnyBlog = getAllowAnyBlog(keywordDoc.sheetType);

      const allMatches = matchBlogs(query, items, { allowAnyBlog });

      const uniqueGroups = new Set(items.map((item: any) => item.group));
      isPopular = uniqueGroups.size === 1;
      uniqueGroupsSize = uniqueGroups.size;
      topicNamesArray = Array.from(uniqueGroups);

      const typeStr = isPopular ? 'Ïù∏Í∏∞Í∏Ä' : 'Ïä§Î∏î';
      progressLogger.newCrawl(searchQuery, items.length, allMatches.length, typeStr);

      crawlCache.set(searchQuery, html);
      itemsCache.set(searchQuery, items);
      matchQueueMap.set(searchQuery, [...allMatches]);
      htmlStructureCache.set(searchQuery, {
        isPopular,
        uniqueGroups: uniqueGroupsSize,
        topicNames: topicNamesArray,
      });

      progressLogger.queueChange(0, allMatches.length, 'init');

      await randomDelay(CRAWL_CONFIG.delayBetweenQueries, CRAWL_CONFIG.delayBetweenQueries * 2);
    } catch (error) {
      logger.error(`Í≤ÄÏÉâÏñ¥ "${searchQuery}" ÌÅ¨Î°§ÎßÅ ÏóêÎü¨: ${(error as Error).message}`);

      const restaurantName = extractRestaurantName(keywordDoc, query);

      progressLogger.failure({
        index: globalIndex,
        total: totalKeywords,
        keyword: query,
        restaurantName,
        reason: 'ÌÅ¨Î°§ÎßÅ ÏóêÎü¨',
      });

      await updateFunction(
        String(keywordDoc._id),
        false,
        '',
        '',
        keywordType,
        restaurantName,
        '',
        0,
        '',
        0,
        false
      );

      const crawlErrorLog = logBuilder.createCrawlError({
        index: globalIndex,
        keyword: query,
        searchQuery,
        restaurantName,
        vendorTarget: '',
        startTime: keywordStartTime,
        error: error as Error,
      });
      logBuilder.push(crawlErrorLog);

      return null;
    }
  } else {
    progressLogger.cacheUsed({
      index: globalIndex,
      total: totalKeywords,
      searchQuery,
    });
    items = itemsCache.get(searchQuery)!;
    const structure = htmlStructureCache.get(searchQuery)!;
    isPopular = structure.isPopular;
    uniqueGroupsSize = structure.uniqueGroups;
    topicNamesArray = structure.topicNames;
  }

  return { items, isPopular, uniqueGroupsSize, topicNamesArray };
};
